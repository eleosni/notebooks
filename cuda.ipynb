{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4597,
     "status": "ok",
     "timestamp": 1631007812640,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "6rW6VOwB772r",
    "outputId": "6423cc1e-3582-45a7-f4e9-67215d21d242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1632295391205,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "8PGRR2NsJbBp",
    "outputId": "b41dfed6-9215-4055-fa19-2ab80dca031f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.51.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numba\n",
    "numba.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2443,
     "status": "ok",
     "timestamp": 1632295373025,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "iR1NXhoaJhux",
    "outputId": "1b29803d-f1e8-4187-bf48-56243a0556cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5946,
     "status": "ok",
     "timestamp": 1632296639222,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "FCjVEpLBJk_y",
    "outputId": "0cb255ea-7a38-421d-8c2b-4cbc5f39c8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without GPU: 5.412367127999914\n",
      "with GPU: 0.2809013410001171\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "# To run on CPU\n",
    "def func(a):\n",
    "    for i in range(10000000):\n",
    "        a[i]+= 1\n",
    "# To run on GPU\n",
    "@jit(nopython=True)\n",
    "def func2(x):\n",
    "    sh = x.shape[0]\n",
    "    b = np.arange(sh).astype(np.int32)    \n",
    "    return x+1+b\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    n = 10000000\n",
    "    a = np.ones(n, dtype = np.float64)\n",
    "    \n",
    "    start = timer()\n",
    "    func(a)\n",
    "    print(\"without GPU:\", timer()-start)\n",
    "    start = timer()\n",
    "    func2(a)\n",
    "    numba.cuda.profile_stop()\n",
    "    print(\"with GPU:\", timer()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1631021020242,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "tyE88Cp1TRK9",
    "outputId": "3aaa6c4f-7bc6-4d28-e9f8-df9251d248cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       7,        8,        9, ..., 10000004, 10000005, 10000006],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "@cuda.jit\n",
    "def increment_by_one(an_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + ty * bw\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] += 7\n",
    "\n",
    "\n",
    "an_array = np.arange(n).astype(np.int32)\n",
    "threadsperblock = 32\n",
    "blockspergrid = (an_array.size + (threadsperblock - 1)) // threadsperblock\n",
    "increment_by_one[blockspergrid, threadsperblock](an_array)\n",
    "\n",
    "an_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIX4aXPsTVm6"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5w9KnqHK9vx"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Note the use of an `out` array. CUDA kernels written with `@cuda.jit` do not return values,\n",
    "# just like their C counterparts. Also, no explicit type signature is required with @cuda.jit\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    \n",
    "    # The actual values of the following CUDA-provided variables for thread and block indices,\n",
    "    # like function parameters, are not known until the kernel is launched.\n",
    "    \n",
    "    # This calculation gives a unique thread index within the entire grid (see the slides above for more)\n",
    "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    #idx = cuda.grid(1)          # 1 = one dimensional thread grid, returns a single value.\n",
    "                                # This Numba-provided convenience function is equivalent to\n",
    "                                # `cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x`\n",
    "\n",
    "    # This thread will do the work on the data element with the same index as its own\n",
    "    # unique index within the grid.\n",
    "    out[idx] = x[idx] + y[idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crv0ckaVLM1p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 4096\n",
    "x = np.arange(n).astype(np.int32) # [0...4095] on the host\n",
    "y = np.ones_like(x)               # [1...1] on the host\n",
    "\n",
    "d_x = cuda.to_device(x) # Copy of x on the device\n",
    "d_y = cuda.to_device(y) # Copy of y on the device\n",
    "d_out = cuda.device_array_like(d_x) # Like np.array_like, but for device arrays\n",
    "\n",
    "# Because of how we wrote the kernel above, we need to have a 1 thread to one data element mapping,\n",
    "# therefore we define the number of threads in the grid (128*32) to equal n (4096).\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1631094405902,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "pTB3Hq6QLOkA",
    "outputId": "90018435-3c0b-4c90-f786-9c102e356c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    2    3 ... 4094 4095 4096]\n"
     ]
    }
   ],
   "source": [
    "add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n",
    "#cuda.synchronize()\n",
    "print(d_out.copy_to_host()) # Should be [1...4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1630915770748,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "ZYi16b-pJ62J",
    "outputId": "ddb21649-d361-4153-a90a-79bd252554c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2 # do the computation\n",
    "\n",
    "# Host code   \n",
    "data = numpy.ones(256)\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1630915812034,
     "user": {
      "displayName": "Святослав Карев",
      "photoUrl": "",
      "userId": "01284637082390164160"
     },
     "user_tz": -420
    },
    "id": "KdswQZmNJ8XR",
    "outputId": "5343e500-6592-4952-bdb6-b4e1ed16a528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144. 144.\n",
      "  144. 144. 144. 144. 144. 144. 144. 144.]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = numpy.full((24, 12), 3, numpy.float) # matrix containing all 3's\n",
    "B = numpy.full((12, 22), 4, numpy.float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((24, 22))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(C)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNqwVlGdLR4ZLRYXY6V61an",
   "collapsed_sections": [],
   "name": "cuda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
